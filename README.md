# Participants-to-items-videos
For those cases where analysing experimental video data per item instead of per participant is desired, this script can automatically rearrange videos for you. It was originally designed to rearrange 16 participant recordings of gesture responses to 234 randomly ordered stimuli into 234 grouped item videos to look for pervasive gestures (i.e., those produced by >50% of the participants). The stimulus presentation was timed, making the spacing between items regular, but with manual input of irregular timestamps, any video file can be processed. It requires two input files: one detailing which clips to cut from which original files (‘trim-input’), and one detailing which clips to concatenate into the item videos (‘concatenate-input’).

Additionally, there is a separate script that can partially generate the trim-input file for you, in case of a regularly timed experiment, based on the filenames and start times you provide. For the original purpose, the input consisted of 16 entries (filename + experiment start time), and resulted in (16x234) 3744 entries of filename + clip start time + duration. These are the first 3 of 4 columns of the trim-input file. We plan to also automate the last column: extracting the randomized item order for each participant from the PsychoPy output, converting those to suitable filenames and matching them to the timestamps.

The initial sample video used to create the sample PP_item and item videos was too large for GitHub, so was not uploaded.
